{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa811eb-4f8e-4cae-99da-34eaa3c70450",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 1: What is Web Scraping? Why is it used? Give three areas where Web Scraping is used to get data.\n",
    "Answer: \n",
    "Web Scraping: web scraping is an method to obtain large amount of data from websites. Most of this data is unstructured data is an HTML format which is then converted into\n",
    "structured data in a spreadsheet or a database so that it can be used in various applications. There are many different ways to perfome web scrapping to obtain data from websites.\n",
    "These include using online services, particular API's or even creating your code for web scraping from scratch.\n",
    "Many large websites, like Google, Twitter, Facebook, StackOverflow, etc. have API's that allow user to access large amounts of data in structured form or they are simply not\n",
    "that technologically advanced. In that situation, it is best to use Web Scrapping to scrape the website for data.\n",
    "\n",
    "Web Scraping requries two parts, namely the crawler and the scraper. the crawler is an artifical intelligence algorithm that browses the web to search for the particular data \n",
    "requried by following the link across the internet. The scraper, on the other hand, is a specific tool created to extract data from the websites. the design of the scraper, on the\n",
    "other hand, is a specific tool created to extract data from the website. the design of the scraper can vary greatly according to the complexity and scope of the project so\n",
    "that it can quickly and accurately extract the data.\n",
    "\n",
    "Why it is used:\n",
    "Suppose you want some information from a website? Let's say a paragraph on Donald Trump! what do you do?\n",
    "Well, you can copy and paste the information from wikipedia to your own file. but what if you want to get large amounts of information from a website as quickly sa possible? Such\n",
    "a large amounts of data from a website to train a Machine Learning algorithm? In such a situation, copying and pasting will not Work! And that's when you 'll need to use \n",
    "Web Scraping. Unlike the long and mind numbing process of manually getting data, Web scrapping uses intelligience automation methods to get thousands or even millions of data sets in \n",
    "a smaller amount of time.\n",
    "\n",
    "Areas to get Web scraping:\n",
    "Web Scraping is used in a variety of digital business that rely on data harvesting. Legitimate use cases include:\n",
    "  ~  Search engine bots crawling asite, analyzing its content and then ranking it.\n",
    "~ Price comparison sites deploying bots to auto- fetch prices and product descriptions for allied seller websites.\n",
    "~ Market research companies using scrapers to pull data from formus and social media(e.g., for sentiment analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47583871-84dd-47b9-8972-d4f3be32f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 2: What are the different methods used for web Scraping?\n",
    "Answer: \n",
    "Data Scraping Techniques:\n",
    "Here are a few techniques commonly used to scrape data from websites, In generally, all web scarping techniques retrieve content from websites, process it using a scraping \n",
    "engine, and generate one or more data files with the extracted content.\n",
    "\n",
    "HTML Parsing\n",
    "HTML parsing involves the use of javascript to targt a linear or nested HTML page. it is a powerful and fast method for extracting text and links(e.g. a nested link or email address),\n",
    "scaping screens and pulling resources.\n",
    "\n",
    "DOM Parsing:\n",
    "The Document Object Model(DOM) defines the structure, style and content of an XML file. Scrapers typically use a DOM parser to view the strucutre of web pages in depth. \n",
    "DOM parsers can be used to access the nodes that contain information and scrape the web page tools like XPath. For dynamically generated content, scrapers can embed web browser like\n",
    "Firefox and internet explore to extract whole pages(or part of them).\n",
    "\n",
    "Vertical Aggregation:\n",
    "Companies that use extensive computing power can create vertical aggreation platforms to target paticular verticals. These are data harvesting platform that can be run on the cloud \n",
    "and are used to automatically generate and monitor bots for certain verticals with minimal human invention. Bots are generated according to the information requried to each\n",
    "vertical, and thier efficency is determined by the quality of data they extract.\n",
    "\n",
    "XPath\n",
    "XPath is short for XML Path Language, which is a query language for XML documents.XML documents have tree-like structures, so scrapers can use XPath to navigation through them\n",
    "by selecting nodes according to various parameters. A scraper may comine DOM parasing with XPath to extract whole web pages and publish them on destination site.\n",
    "\n",
    "Google Sheets\n",
    "Google Sheets is a popular tool for data scraping. Scrapers can use the IMPORTXML function in sheets to scrape from a website, which is useful if they want to extract a \n",
    "specific pattern or data from the website. this command also makes it possible to check if a website can be scraped or is protected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05221a1d-2de3-420e-9c28-fa28928a2ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 3: What is Beautiful Soup? Why is it used?\n",
    "Answer: \n",
    "Beautiful Soup:\n",
    "Beautiful soup is a python library that makes it easy to srcape information from web pages. It sits atop an HTML or XML parser and provides Pythonic idioms for iterating, searching, \n",
    "modifying the parse tree.\n",
    "\n",
    "Features of Beautiful Soup:\n",
    "Some Key features that makes beautiful soup unique are:\n",
    "1.) Beautiful soup provides a few simple methods and pythonic idioms for navigation, searching, and modifying a parse tree.\n",
    "2.) Beautiful soup automatically converts incoming documents to Unicode and outgoing documents to UTF-8.\n",
    "3.) Beautiful Soup sits on top of popular python parsers like lxml and htmll5lib, which allows us to try out different parsing strategies or trade speed for flexibility.\n",
    "\n",
    "Uses of Beautiful Soup:\n",
    "The beautiful soup library helps with isolating titles and links from webpages. it can extract all of the text from HTML tags, and alter the HTML in the document with which \n",
    "we 're working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183126ea-365c-4b0f-ad5b-5d2e47f31be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 4: Why is flask used in the web scraping project?\n",
    "Answer: \n",
    "Flask is a lightweight framework to bulid websites. we will use to prase our collected data and display it as HTML in a new HTML file.\n",
    "The requests module allows us to send http requests to the websites we want to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f871fdc1-0141-4fa6-83ff-697b8f9a9a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 5: Write the names of AWS services used in this project. Also, explain the use of each service.\n",
    "Answer: \n",
    "Aws is a cloud computing platform supported by Amazon that includes a mixture of infrastrucutre as a service(IaaS), platform as a service(Paas), and packaged softare as a service\n",
    "(SaaS) offerings. \n",
    "In easy words, AWS lets you run complex programs or services on thier servers so that you do not need a cutting-edge computer. There are lots of services you can choose from.\n",
    "Among them, tody we need Lambda, EventBridge, Dynamodh, and Identity and access Management(IAM) to web scrape and save data\n",
    "\n",
    "What is AWS Lambda, EventBridge, DynamoDB, IAM\n",
    "\n",
    "Lambda\n",
    "in shorts, AWS Lambda lets you write or upload a code and dependencies so that you can use Amazon Infrastructyre to run the code instead of with your computer. The benifit of this\n",
    "is that you can run any code regardless of your computer specifics and do not need to turn on your computer to run your code. Also, you can integrate it with other AWS services to do\n",
    "more advanced stuff.\n",
    "\n",
    "EventBridge:\n",
    "on the offical document, it says that AWS EventBridge lets you connect your applications with data from a variety of sources. This means it detects an event from other services in \n",
    "real-time and activate an action when it detects an event. so with this service, you can easily set up connections between services or monitor your process.\n",
    "\n",
    "DynamoDB:\n",
    "Dynamodb is a NoSQL databse service you can use from AWS. and because AWS takes care of the configuration process of a database, you can easily create tables, store data, and query \n",
    "rows. one of the benfits of using this is scalability since it uses the cloud to store data.\n",
    "\n",
    "IAM\n",
    "However, these services cannot be accessed from the others by defaukt for security "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
